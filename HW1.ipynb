{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c906be3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c9e58e2",
   "metadata": {
    "code_folding": [
     2
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MLP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        linear_1_in_features,\n",
    "        linear_1_out_features,\n",
    "        f_function,\n",
    "        linear_2_in_features,\n",
    "        linear_2_out_features,\n",
    "        g_function\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            linear_1_in_features: the in features of first linear layer\n",
    "            linear_1_out_features: the out features of first linear layer\n",
    "            linear_2_in_features: the in features of second linear layer\n",
    "            linear_2_out_features: the out features of second linear layer\n",
    "            f_function: string for the f function: relu | sigmoid | identity\n",
    "            g_function: string for the g function: relu | sigmoid | identity\n",
    "        \"\"\"\n",
    "        self.f_function = f_function\n",
    "        self.g_function = g_function\n",
    "\n",
    "        self.parameters = dict(\n",
    "            W1 = torch.randn(linear_1_out_features, linear_1_in_features),\n",
    "            b1 = torch.randn(linear_1_out_features),\n",
    "            W2 = torch.randn(linear_2_out_features, linear_2_in_features),\n",
    "            b2 = torch.randn(linear_2_out_features),\n",
    "        )\n",
    "        self.grads = dict(\n",
    "            dJdW1 = torch.zeros(linear_1_out_features, linear_1_in_features),\n",
    "            dJdb1 = torch.zeros(linear_1_out_features),\n",
    "            dJdW2 = torch.zeros(linear_2_out_features, linear_2_in_features),\n",
    "            dJdb2 = torch.zeros(linear_2_out_features),\n",
    "        )\n",
    "        \n",
    "\n",
    "        # put all the cache value you need in self.cache\n",
    "        self.cache = dict()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor shape (batch_size, linear_1_in_features)\n",
    "        \"\"\"\n",
    "       \n",
    "        \n",
    "        # TODO: Implement the forward function\n",
    "        \n",
    "        h1 = x@self.parameters['W1'].t() + self.parameters['b1']\n",
    "        self.cache['h1'] = h1\n",
    "        if self.f_function == 'relu':\n",
    "            z1 = F.relu(h1)\n",
    "            \n",
    "        if self.f_function == 'sigmoid':\n",
    "            z1 = F.sigmoid(h1)\n",
    "            \n",
    "        if self.f_function == 'identity':\n",
    "            z1 = h1\n",
    "            \n",
    "        self.cache['z1'] = z1\n",
    "        \n",
    "        h2 = z1@self.parameters['W2'].t() + self.parameters['b2']\n",
    "            \n",
    "        if self.g_function == 'relu':\n",
    "            z2 = F.relu(h2)\n",
    "            \n",
    "        if self.g_function == 'sigmoid':\n",
    "            z2 = F.sigmoid(h2)\n",
    "            \n",
    "        if self.g_function == 'identity':\n",
    "            z2 = h2\n",
    "            \n",
    "        return z2\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dJdy_hat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the backward function\n",
    "        #flow: x -> linear1(w,b,x) ->h1 -> activation1(h1) -> z1 -> linear2(w2,b2,z1)-> h2 -> activation2(h2) -> y_hat\n",
    "        \n",
    "        #g_function grads\n",
    "        if self.g_function == 'relu':\n",
    "            dAct2 = (h2>0).float() * dJdy_hat\n",
    "        if self.g_function == 'sigmoid':\n",
    "            #dsigmoid = sigmoid(1-sigmoid)\n",
    "            dAct2 = 1/(1+exp(-h2))(1 - (1/(1+exp(-h2))))\n",
    "        else:\n",
    "            dAct2 = dJdy_hat\n",
    "        \n",
    "        #linear 2 grads\n",
    "        \n",
    "        #print(self.cache['z1'].size(),dAct2.size())\n",
    "\n",
    "        #self.param transposed in forward pose so A.t().t() = A\n",
    "        dZ1 = dAct2 @ self.parameters['W2']\n",
    "        \n",
    "        #print(self.cache['z1'].size(),dAct2.size())\n",
    "        #print(self.cache['z1'].unsqueeze(-1).size(), 'act2unsq', dAct2.unsqueeze(1).size())\n",
    "        dJdW2 = (self.cache['z1'].unsqueeze(-1) * dAct2.unsqueeze(1)).sum(0)\n",
    "        dJdb2 = dAct2.sum(0)\n",
    "    \n",
    "        #f_function grads\n",
    "        if self.f_function == 'relu':\n",
    "            dAct1 = (self.cache['h1']>0).float() * dZ1\n",
    "        if self.f_function == 'sigmoid':\n",
    "            #dsigmoid = sigmoid(1-sigmoid)\n",
    "            dAct1 = 1/(1+exp(-self.cache['h1']))(1 - (1/(1+exp(-self.cache['h1']))))\n",
    "        else:\n",
    "            dAct1 = dZ1\n",
    "        \n",
    "        #linear 1 grads\n",
    "        \n",
    "        #self.param transposed in forward pose so A.t().t() = A\n",
    "        #dx = dAct2 @ self.parameters['W1']\n",
    "        dJdW1 = (x.unsqueeze(-1) * dAct1.unsqueeze(1)).sum(0)\n",
    "        dJdb1 = dAct1.sum(0)     \n",
    "\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def clear_grad_and_cache(self):\n",
    "        for grad in self.grads:\n",
    "            self.grads[grad].zero_()\n",
    "        self.cache = dict()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66075e98",
   "metadata": {
    "code_folding": [
     6
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mse_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14652/141643330.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_grad_and_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdJdy_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdJdy_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mse_loss' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "net = MLP(\n",
    "    linear_1_in_features=2,\n",
    "    linear_1_out_features=20,\n",
    "    f_function='relu',\n",
    "    linear_2_in_features=20,\n",
    "    linear_2_out_features=5,\n",
    "    g_function='identity'\n",
    ")\n",
    "x = torch.randn(10, 2)\n",
    "y = torch.randn(10, 5)\n",
    "\n",
    "\n",
    "#Forward: (10,2)@(2,20)@(20,5) -> (10,5)\n",
    "#Backward: dJdy_hat:(10,5)\n",
    "\n",
    "net.clear_grad_and_cache()\n",
    "y_hat = net.forward(x)\n",
    "J, dJdy_hat = mse_loss(y, y_hat)\n",
    "\n",
    "net.backward(dJdy_hat)\n",
    "\n",
    "# y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e8a158",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## loss v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f202317e",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mse_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: the label tensor (batch_size, linear_2_out_features)\n",
    "        y_hat: the prediction tensor (batch_size, linear_2_out_features)\n",
    "\n",
    "    Return:\n",
    "        J: scalar of loss\n",
    "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the mse loss\n",
    "    J = ((y - y_hat).pow(2)).mean()\n",
    "    dJdy_hat = 2*(y - y_hat)\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "    return J, dJdy_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd543677",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# compare the result with autograd\n",
    "net_autograd = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('linear1', nn.Linear(2, 20)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('linear2', nn.Linear(20, 5)),\n",
    "    ])\n",
    ")\n",
    "net_autograd.linear1.weight.data = net.parameters['W1']\n",
    "net_autograd.linear1.bias.data = net.parameters['b1']\n",
    "net_autograd.linear2.weight.data = net.parameters['W2']\n",
    "net_autograd.linear2.bias.data = net.parameters['b2']\n",
    "\n",
    "y_hat_autograd = net_autograd(x)\n",
    "\n",
    "J_autograd = F.mse_loss(y_hat_autograd, y)\n",
    "\n",
    "net_autograd.zero_grad()\n",
    "J_autograd.backward()\n",
    "\n",
    "print((net_autograd.linear1.weight.grad.data - net.grads['dJdW1']).norm() < 1e-3)\n",
    "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.weight.grad.data - net.grads['dJdW2']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm()< 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20086dec",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bce_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_hat: the prediction tensor\n",
    "        y: the label tensor\n",
    "        \n",
    "    Return:\n",
    "        loss: scalar of loss\n",
    "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the bce loss\n",
    "    pass\n",
    "\n",
    "    # return loss, dJdy_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98bddb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Bigger question is why the course has decided to set up the net this way for the first exercise. Maybe this is more intuitive another way, or I've been spoiled by already somewhat knowing what route pytorch goes, but storing the weight matrices in a dict and then having activation functions callable by the test suite just seems to make an ugly mess out of things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f411e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What if we try not write this weird dictionary version of the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd89a9f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Okay, so having fiddled around a bit more, I'm beginning to understand why ```self.parameters``` as a dict works. For separate forward and backward passes, you want the parameters to be globally available to the MLP class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a2309",
   "metadata": {},
   "source": [
    "# V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a355908a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        linear_1_in_features,\n",
    "        linear_1_out_features,\n",
    "        f_function,\n",
    "        linear_2_in_features,\n",
    "        linear_2_out_features,\n",
    "        g_function\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            linear_1_in_features: the in features of first linear layer\n",
    "            linear_1_out_features: the out features of first linear layer\n",
    "            linear_2_in_features: the in features of second linear layer\n",
    "            linear_2_out_features: the out features of second linear layer\n",
    "            f_function: string for the f function: relu | sigmoid | identity\n",
    "            g_function: string for the g function: relu | sigmoid | identity\n",
    "            \n",
    "        \"\"\"\n",
    "        self.parameters = dict(\n",
    "            W1 = torch.randn(linear_1_out_features,linear_1_in_features),\n",
    "            b1 = torch.randn(linear_1_out_features),\n",
    "            W2 = torch.randn(linear_2_out_features,linear_2_in_features),\n",
    "            b2 = torch.randn(linear_2_out_features),\n",
    "        )\n",
    "        self.grads = dict(\n",
    "            dJdW1 = torch.zeros(linear_1_out_features, linear_1_in_features),\n",
    "            dJdb1 = torch.zeros(linear_1_out_features),\n",
    "            dJdW2 = torch.zeros(linear_2_out_features, linear_2_in_features),\n",
    "            dJdb2 = torch.zeros(linear_2_out_features),\n",
    "        )\n",
    "        \n",
    "        self.f_function = f_function\n",
    "        self.g_function = g_function\n",
    "        \n",
    "        # put all the cache value you need in self.cache\n",
    "        self.cache = dict()\n",
    "    \n",
    "    def linear(self, inp, W, b):\n",
    "        return inp@W.t() + b #torch convention does linear as W.t() - see defn of W above as W = (out, in)\n",
    "    \n",
    "    def mse(self, output, targ): \n",
    "        return (output.squeeze(-1) - targ).pow(2).mean()          \n",
    "        \n",
    "    def lin_grad(self, inp, out, w, b):\n",
    "        # grad of matmul with respect to input\n",
    "        #print('out.g', out.g.size(),'squeeeze', out.g.squeeze().size())\n",
    "        \n",
    "        inp.g = out.g.squeeze() @ w  #w.t().t()\n",
    "        #print(\"input\",inp.unsqueeze(-1).size(),\"output\", out.g.squeeze().unsqueeze(1).size()) #for discussion of matrix dimensions see here https://forums.fast.ai/t/lesson-8-2019-discussion-wiki/41323/642\n",
    "        \n",
    "        #in brief: input (10,20) is batch size 10 with 20 features computed by prev linear layer. \n",
    "        #Unsqueeze adds a dim to make it (10,20,1), and unsqueeze on out.g makes it (10,1,5)\n",
    "        #reason for out.g.squeeze().unsqueeze is because out.g for linear 2 was unsqueezed in mse\n",
    "        #multipyling the two matrices gives result of (10,20,5) i.e. 10 (20,5) weight matrices\n",
    "        #sum over dim(0) here then aggregates the contributions of our gradients for each of the 10 examples\n",
    "        \n",
    "        w.g = (inp.unsqueeze(-1) * out.g.squeeze().unsqueeze(1)).sum(0) #sum(0) is to sum the gradients over the batch afaik\n",
    "        b.g = out.g.sum(0) \n",
    "        #capture grad info for comparison to torch\n",
    "        \n",
    "        #print(\"dJdW1\",self.grads['dJdW2'].size(), \"w.g\", w.g.t().size(), \"w1\", self.parameters['W1'].size(), \"w\", w.size())\n",
    "        #print(str(w),\\n,str(self.parameters['W2']))\n",
    "        if str(w) == str(self.parameters['W2']):\n",
    "            self.grads['dJdW2'] = w.g.t()\n",
    "            print(\"before setting\",self.grads['dJdb2'])\n",
    "            new_bg = torch.zeros(b.size())\n",
    "             \n",
    "            for i in range(b.g.numel()):\n",
    "                new_bg[i] = float(b.g[i])\n",
    "            self.grads['dJdb2'] = new_bg\n",
    "            print(\"after setting\",self.grads['dJdb2'])\n",
    "        if str(w) == str(self.parameters['W1']):\n",
    "            self.grads['dJdW1'] = w.g.t()\n",
    "            self.grads['dJdb1'] = b.g\n",
    "\n",
    "    def relu_grad(self, inp, out):\n",
    "        inp.g = (inp>0).float() * out.g\n",
    "\n",
    "    def mse_grad(self, inp, targ): \n",
    "        # grad of loss with respect to output of previous layer\n",
    "        inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.numel()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor shape (batch_size, linear_1_in_features)\n",
    "        \"\"\"\n",
    "        self.cache['x'] = x\n",
    "        h1 = self.linear(x,self.parameters['W1'],self.parameters['b1'])\n",
    "        self.cache['h1'] = h1\n",
    "        z1 = F.relu(h1)\n",
    "        self.cache['z1'] = z1\n",
    "        \n",
    "#         print('z1size1', z1.size(),'z1cache',self.cache['z1'].size())\n",
    "        \n",
    "        h2 = self.linear(z1,self.parameters['W2'],self.parameters['b2'])\n",
    "        self.cache['h2'] = h2\n",
    "        y_hat = h2 #g_function is identity\n",
    "        self.cache['y_hat'] = y_hat\n",
    "        loss = self.mse(y_hat,y)\n",
    "        \n",
    "        #print(\"forward grads\", self.grads['dJdb2'])\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def backward(self, y):\n",
    "\n",
    "        self.mse_grad(self.cache['y_hat'], y)\n",
    "        # activation 2 is Identity\n",
    "        self.lin_grad(self.cache['z1'], self.cache['h2'], self.parameters['W2'], self.parameters['b2']) #linear 2\n",
    "        #print(\"backward grads\", self.grads['dJdb2'])\n",
    "        self.relu_grad(self.cache['h1'], self.cache['z1']) #act 1\n",
    "        self.lin_grad(self.cache['x'], self.cache['h1'], self.parameters['W1'], self.parameters['b1'])   #linear 1\n",
    "\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def clear_grad_and_cache(self):\n",
    "        for grad in self.grads:\n",
    "            self.grads[grad].zero_()\n",
    "        self.cache = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6870d0fd",
   "metadata": {
    "code_folding": [
     0,
     5,
     25
    ]
   },
   "outputs": [],
   "source": [
    "def mse_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: the label tensor (batch_size, linear_2_out_features)\n",
    "        y_hat: the prediction tensor (batch_size, linear_2_out_features)\n",
    "    Return:\n",
    "        loss: scalar of loss\n",
    "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement the mse loss\n",
    "    # batch_size = y.size(0)\n",
    "    # linear_2_out_features = y.size(1)\n",
    "    loss = sum(sum((y-y_hat).pow(2)))\n",
    "    dJdy_hat = 2*(y_hat-y)\n",
    "    # taking mean\n",
    "    loss = loss/y.numel()\n",
    "    dJdy_hat = dJdy_hat/y.numel()\n",
    "    return loss, dJdy_hat\n",
    "\n",
    "\n",
    "def mse(output, targ): \n",
    "    return (output.squeeze(-1) - targ).pow(2).mean() \n",
    "\n",
    "\n",
    "def mse_grad(inp, targ): \n",
    "    # grad of loss with respect to output of previous layer\n",
    "    inp.g = 2 * (inp - targ) / inp.numel() #the fast.ai version dividing by inp.shape[0] here doesn't work because in this we have a batch size so can't divide by the number of entries in our input vector but most divide by number of entries in our input TENSOR! (hence need .numel())\n",
    "    return inp.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "860d7f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "torch.Size([5, 5])\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "test = torch.randn(5,5)\n",
    "targ = torch.randn(5,5)\n",
    "\n",
    "\n",
    "print(targ.numel())\n",
    "print(targ.size())\n",
    "\n",
    "loss, dJdy_hat = mse_loss(targ,test)\n",
    "\n",
    "print((loss - mse(test,targ)).norm() < 1e-3)\n",
    "\n",
    "print((dJdy_hat - mse_grad(test,targ)).norm() < 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0bdf09",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ffdf4786",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before setting tensor([0., 0., 0., 0., 0.])\n",
      "after setting tensor([-0.7399, -1.1462, -3.6524,  5.6012,  1.4159])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "net = MLP(\n",
    "    linear_1_in_features=2,\n",
    "    linear_1_out_features=20,\n",
    "    f_function='relu',\n",
    "    linear_2_in_features=20,\n",
    "    linear_2_out_features=5,\n",
    "    g_function='identity'\n",
    ")\n",
    "x = torch.randn(10, 2) #means batch size of 10, input features 2\n",
    "y = torch.randn(10, 5) #means 10 inputs classified between 5 outputs\n",
    "\n",
    "net.clear_grad_and_cache()\n",
    "y_hat = net.forward(x,y)\n",
    "\n",
    "net.backward(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9a3a7ef8",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# compare the result with autograd\n",
    "net_autograd = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('linear1', nn.Linear(2,20)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('linear2', nn.Linear(20,5)),\n",
    "    ])\n",
    ")\n",
    "net_autograd.linear1.weight.data = net.parameters['W1']\n",
    "net_autograd.linear1.bias.data = net.parameters['b1']\n",
    "net_autograd.linear2.weight.data = net.parameters['W2']\n",
    "net_autograd.linear2.bias.data = net.parameters['b2']\n",
    "\n",
    "y_hat_autograd = net_autograd(x)\n",
    "\n",
    "J_autograd = F.mse_loss(y_hat_autograd, y)\n",
    "\n",
    "net_autograd.zero_grad()\n",
    "J_autograd.backward()\n",
    "\n",
    "\n",
    "print((net_autograd.linear1.weight.grad.data - net.grads['dJdW1']).norm() < 1e-3)\n",
    "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.weight.grad.data - net.grads['dJdW2']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.bias.grad.data.t() - net.grads['dJdb2']).norm() < 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74adff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7399, -1.1462, -3.6524,  5.6012,  1.4159])\n",
      "tensor([-0.7399, -1.1462, -3.6524,  5.6012,  1.4159])\n"
     ]
    }
   ],
   "source": [
    "print(net.grads['dJdb2'])\n",
    "print(net_autograd.linear2.bias.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164e996",
   "metadata": {},
   "source": [
    "currently unsure why I'm failing these tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574cb00",
   "metadata": {},
   "source": [
    "Turns out it was literally 1 line in the code - normalising the mse_grad by ``.shape[0]`` rather than by ``numel()``.\n",
    "\n",
    "The reason for my mistake was not thinking clearly about the batch size - I was implicitly dealing with a batch size of 1 but that doesn't make sense for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca396d0b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dJdW1': tensor([[  6.3688,  -0.5613],\n",
      "        [ -4.2895,   2.2825],\n",
      "        [  5.5466,  -2.0888],\n",
      "        [ -1.5885,  -1.8586],\n",
      "        [ -7.1742,   4.1545],\n",
      "        [ -9.2972,   6.8226],\n",
      "        [ -1.6694,   1.2718],\n",
      "        [  7.2225,  -1.4345],\n",
      "        [ -1.4697,  -1.3303],\n",
      "        [  6.5135,  -3.5458],\n",
      "        [ -2.8434,   0.3092],\n",
      "        [ -8.2150,   4.7369],\n",
      "        [-11.7968,   3.2561],\n",
      "        [ 18.2542,  -2.8796],\n",
      "        [-12.3716,   6.5908],\n",
      "        [ 12.5386,  -4.8811],\n",
      "        [ -8.1859,   4.6049],\n",
      "        [ -7.3827,  -0.1397],\n",
      "        [ -5.4608,   3.4130],\n",
      "        [ 17.6165,  -3.1712]]), 'dJdb1': tensor([  5.8247,   4.5963,  -3.6311,  -8.3613,   5.2256,  17.8559,  -1.9851,\n",
      "          7.3082, -12.4775,  -4.5635,   1.6294,  18.2070,  10.4480,  20.4293,\n",
      "          7.5341,  17.7802,   4.4985,   9.2700,   1.7760,  22.0492]), 'dJdW2': tensor([[ 3.2155e+00,  4.5823e+00,  1.7658e+00,  3.2703e+00,  8.3859e+00,\n",
      "          7.7257e+00,  3.2235e+00,  1.0451e+00,  6.5746e+00,  2.4196e+00,\n",
      "          9.9506e+00,  7.2239e+00,  1.4938e+01,  2.4504e+00,  5.1350e+00,\n",
      "          2.6618e+00,  6.9584e-04,  8.5287e+00,  3.1897e+00,  6.2015e+00],\n",
      "        [-2.6208e+00, -4.7261e+00, -1.4357e+00, -5.0881e+00, -8.0212e+00,\n",
      "         -5.8668e+00, -1.7003e+00, -6.6152e-01, -7.1160e+00, -2.1814e+00,\n",
      "         -1.0450e+01, -8.0956e+00, -1.4179e+01, -1.1805e+00, -4.5049e+00,\n",
      "         -2.3677e+00, -5.2872e-04, -9.0664e+00, -3.6415e+00, -4.2974e+00],\n",
      "        [-8.0516e-01, -6.0170e+00, -2.5283e+00, -5.1889e+00, -1.1771e+01,\n",
      "         -9.9470e+00, -7.1601e-02,  1.8861e-01, -7.6224e+00, -3.4197e+00,\n",
      "         -1.4047e+01, -7.5128e+00, -2.0478e+01, -4.9137e-01, -7.2710e+00,\n",
      "         -4.5154e-01, -9.9404e-04, -9.3142e+00, -4.6493e+00, -2.0751e-01],\n",
      "        [-3.7870e+00, -2.2780e+00, -1.5301e+00,  1.5648e-01, -5.0145e+00,\n",
      "         -5.9683e+00, -4.4494e+00, -1.6185e+00, -3.3897e+00, -1.6998e+00,\n",
      "         -4.7604e+00, -3.8351e+00, -8.2873e+00, -2.8958e+00, -3.9368e+00,\n",
      "         -3.3565e+00, -7.5596e-04, -4.7475e+00, -1.0537e+00, -8.7395e+00],\n",
      "        [-1.2308e+01, -8.3492e+00, -2.7171e+00, -6.4255e+00, -1.1544e+01,\n",
      "         -1.3356e+01, -1.0696e+01, -4.1337e+00, -1.4761e+01, -3.4596e+00,\n",
      "         -1.3096e+01, -1.8579e+01, -2.3216e+01, -6.9266e+00, -7.5175e+00,\n",
      "         -1.1400e+01, -1.1414e-03, -2.0188e+01, -4.2066e+00, -2.3971e+01]]), 'dJdb2': tensor([[  6.0703],\n",
      "        [ -6.0311],\n",
      "        [ -6.1180],\n",
      "        [ -3.9100],\n",
      "        [-14.8778]])}\n"
     ]
    }
   ],
   "source": [
    "print(net.grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af55769e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(43.5475)\n"
     ]
    }
   ],
   "source": [
    "print(net.grads['dJdW1'].norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c431fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.7095)\n"
     ]
    }
   ],
   "source": [
    "print(net_autograd.linear1.weight.grad.data.norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993cb206",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5ec676",
   "metadata": {
    "code_folding": [
     119
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        linear_1_in_features,\n",
    "        linear_1_out_features,\n",
    "        f_function,\n",
    "        linear_2_in_features,\n",
    "        linear_2_out_features,\n",
    "        g_function\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            linear_1_in_features: the in features of first linear layer\n",
    "            linear_1_out_features: the out features of first linear layer\n",
    "            linear_2_in_features: the in features of second linear layer\n",
    "            linear_2_out_features: the out features of second linear layer\n",
    "            f_function: string for the f function: relu | sigmoid | identity\n",
    "            g_function: string for the g function: relu | sigmoid | identity\n",
    "        \"\"\"\n",
    "        self.f_function = f_function\n",
    "        self.g_function = g_function\n",
    "\n",
    "        self.parameters = dict(\n",
    "            W1 = torch.randn(linear_1_out_features, linear_1_in_features),\n",
    "            b1 = torch.randn(linear_1_out_features),\n",
    "            W2 = torch.randn(linear_2_out_features, linear_2_in_features),\n",
    "            b2 = torch.randn(linear_2_out_features),\n",
    "        )\n",
    "        self.grads = dict(\n",
    "            dJdW1 = torch.zeros(linear_1_out_features, linear_1_in_features),\n",
    "            dJdb1 = torch.zeros(linear_1_out_features),\n",
    "            dJdW2 = torch.zeros(linear_2_out_features, linear_2_in_features),\n",
    "            dJdb2 = torch.zeros(linear_2_out_features),\n",
    "        )\n",
    "\n",
    "        # put all the cache value you need in self.cache\n",
    "        self.cache = dict(\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor shape (batch_size, linear_1_in_features)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward function\n",
    "        w1 = self.parameters['W1']\n",
    "        b1 = self.parameters['b1']\n",
    "        w2 = self.parameters['W2']\n",
    "        b2 = self.parameters['b2']\n",
    "        \n",
    "        self.z1 = w1.mm(x.t())\n",
    "        for i in range(10):\n",
    "          self.z1[:,i] = self.z1[:,i] + b1\n",
    "\n",
    "        if self.f_function == 'identity':\n",
    "          self.z2 = self.z1\n",
    "        elif self.f_function == 'relu':\n",
    "          self.z2 = F.relu_(self.z1)\n",
    "        else:\n",
    "          self.z2 = F.sigmoid(self.z1)\n",
    "        \n",
    "        self.z3 = w2.mm(self.z2)\n",
    "        for i in range(10):\n",
    "          self.z3[:,i] = self.z3[:,i] + b2\n",
    "        if self.g_function == 'relu':\n",
    "          self.y_hat = F.relu_(self.z3)\n",
    "        elif self.g_function == 'identity':\n",
    "          self.y_hat = self.z3\n",
    "        elif self.g_function == 'sigmoid':\n",
    "          self.y_hat = F.sigmoid(self.z3)\n",
    "        return self.y_hat\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dJdy_hat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the backward function\n",
    "        if self.g_function == 'identity':\n",
    "          dy_hatdz3 = 1\n",
    "        elif self.g_function == 'sigmoid':\n",
    "          a = 1 - F.sigmoid(self.z3)\n",
    "          dy_hatdz3 = F.sigmoid(self.z3) * a\n",
    "        elif self.g_function == 'relu':\n",
    "          dy_hatdz3 = torch.sign(F.relu_(self.z3))\n",
    "        \n",
    "        self.grads['dJdW2'] = (dJdy_hat * dy_hatdz3).mm(self.z2.t())/dJdy_hat.size(0)/dJdy_hat.size(1)\n",
    "        self.grads['dJdb2'] = (dJdy_hat * dy_hatdz3).sum(1)/dJdy_hat.size(0)/dJdy_hat.size(1)\n",
    "\n",
    "\n",
    "        dz3dz2 = self.parameters['W2']\n",
    "\n",
    "\n",
    "        if self.f_function == 'identity':\n",
    "          dz2dz1 = 1\n",
    "        elif self.f_function == 'sigmoid':\n",
    "          b = 1 - F.sigmoid(self.z1)\n",
    "          dz2dz1 = F.sigmoid(self.z1) * b\n",
    "        elif self.f_function == 'relu':\n",
    "          dz2dz1 = torch.sign(F.relu_(self.z1))\n",
    "        \n",
    "        self.grads['dJdW1'] = (dz3dz2.t().mm(dJdy_hat * dy_hatdz3) * dz2dz1).mm(x)/dJdy_hat.size(0)/dJdy_hat.size(1)\n",
    "        self.grads['dJdb1'] = (dz3dz2.t().mm(dJdy_hat * dy_hatdz3) * dz2dz1).sum(1)/dJdy_hat.size(0)/dJdy_hat.size(1)\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def clear_grad_and_cache(self):\n",
    "        for grad in self.grads:\n",
    "            self.grads[grad].zero_()\n",
    "        self.cache = dict()\n",
    "\n",
    "def mse_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: the label tensor (batch_size, linear_2_out_features)\n",
    "        y_hat: the prediction tensor (batch_size, linear_2_out_features)\n",
    "    Return:\n",
    "        J: scalar of loss\n",
    "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the mse loss\n",
    "    loss_matrix = 0.5 * (y.t()-y_hat).pow(2).sum(1)\n",
    "    dim = y.size(0)\n",
    "    J = loss_matrix/dim\n",
    "    dJdy_hat = y_hat - y.t()\n",
    "    return J, dJdy_hat\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccb36d0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "net = MLP(\n",
    "    linear_1_in_features=2,\n",
    "    linear_1_out_features=20,\n",
    "    f_function='relu',\n",
    "    linear_2_in_features=20,\n",
    "    linear_2_out_features=5,\n",
    "    g_function='identity'\n",
    ")\n",
    "x = torch.randn(10, 2)\n",
    "y = torch.randn(10, 5)\n",
    "\n",
    "\n",
    "#Forward: (10,2)@(2,20)@(20,5) -> (10,5)\n",
    "#Backward: dJdy_hat:(10,5)\n",
    "\n",
    "net.clear_grad_and_cache()\n",
    "y_hat = net.forward(x)\n",
    "J, dJdy_hat = mse_loss(y, y_hat)\n",
    "\n",
    "net.backward(dJdy_hat)\n",
    "\n",
    "\n",
    "# compare the result with autograd\n",
    "net_autograd = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('linear1', nn.Linear(2, 20)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('linear2', nn.Linear(20, 5)),\n",
    "    ])\n",
    ")\n",
    "net_autograd.linear1.weight.data = net.parameters['W1']\n",
    "net_autograd.linear1.bias.data = net.parameters['b1']\n",
    "net_autograd.linear2.weight.data = net.parameters['W2']\n",
    "net_autograd.linear2.bias.data = net.parameters['b2']\n",
    "\n",
    "y_hat_autograd = net_autograd(x)\n",
    "\n",
    "J_autograd = F.mse_loss(y_hat_autograd, y)\n",
    "\n",
    "net_autograd.zero_grad()\n",
    "J_autograd.backward()\n",
    "\n",
    "print((net_autograd.linear1.weight.grad.data - net.grads['dJdW1']).norm() < 1e-3)\n",
    "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.weight.grad.data - net.grads['dJdW2']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm()< 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cecac6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1512f3dc",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MLP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        linear_1_in_features,\n",
    "        linear_1_out_features,\n",
    "        f_function,\n",
    "        linear_2_in_features,\n",
    "        linear_2_out_features,\n",
    "        g_function\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            linear_1_in_features: the in features of first linear layer\n",
    "            linear_1_out_features: the out features of first linear layer\n",
    "            linear_2_in_features: the in features of second linear layer\n",
    "            linear_2_out_features: the out features of second linear layer\n",
    "            f_function: string for the f function: relu | sigmoid | identity\n",
    "            g_function: string for the g function: relu | sigmoid | identity\n",
    "        \"\"\"\n",
    "        self.f_function = f_function\n",
    "        self.g_function = g_function\n",
    "\n",
    "        self.parameters = dict(\n",
    "            W1 = torch.randn(linear_1_out_features, linear_1_in_features),\n",
    "            b1 = torch.randn(linear_1_out_features),\n",
    "            W2 = torch.randn(linear_2_out_features, linear_2_in_features),\n",
    "            b2 = torch.randn(linear_2_out_features),\n",
    "        )\n",
    "        self.grads = dict(\n",
    "            dJdW1 = torch.zeros(linear_1_out_features, linear_1_in_features),\n",
    "            dJdb1 = torch.zeros(linear_1_out_features),\n",
    "            dJdW2 = torch.zeros(linear_2_out_features, linear_2_in_features),\n",
    "            dJdb2 = torch.zeros(linear_2_out_features),\n",
    "        )\n",
    "\n",
    "        # put all the cache value you need in self.cache\n",
    "        self.cache = dict()\n",
    "\n",
    "\n",
    "    def activation(self, z, type):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: tensor shape (batch_size, linear_out_features)\n",
    "            type: string: relu | sigmoid | identity\n",
    "        Return:\n",
    "            z: tensor shape (batch_size, linear_out_features)\n",
    "            dzdz: element-wisely gradient tensor (batch_size, linear_out_features)\n",
    "        \"\"\"\n",
    "        if type == 'relu': # ReLU fct and its gradient\n",
    "            dzdz = torch.zeros(z.size())\n",
    "            dzdz[z>=0] = 1\n",
    "            z[z<0]=0\n",
    "        elif type == 'sigmoid': # sigmoid fct and its gradient\n",
    "            # dzdz = torch.exp(-z) / (1 + torch.exp(-z)).pow(2)\n",
    "            dzdz = 1 / (2 + torch.exp(-z) + torch.exp(z))\n",
    "            z = 1 / (1 + torch.exp(-z))\n",
    "        else: # identity fct and its gradient\n",
    "            dzdz = torch.ones(z.size())\n",
    "        return z, dzdz\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor shape (batch_size, linear_1_in_features)\n",
    "        Return:\n",
    "            y_hat: the prediction tensor (batch_size, linear_2_out_features)\n",
    "        \"\"\"\n",
    "        # Implement the forward function, cache internal gradients\n",
    "        self.cache['x'] = x\n",
    "        # linear_1\n",
    "        z = torch.mm(x, (self.parameters['W1']).t()) + self.parameters['b1'] # tensor (batch_size, linear_1_out_features)\n",
    "        # f_function\n",
    "        z, dzdz = self.activation(z, self.f_function) # z: changes, dzdz: gradient tensor (batch_size, linear_1_out_features)\n",
    "        self.cache['dzdz']=dzdz\n",
    "        self.cache['z']=z\n",
    "        # linear_2\n",
    "        y_hat = torch.mm(z, (self.parameters['W2']).t()) + self.parameters['b2'] # tensor (batch_size, linear_2_out_features)\n",
    "        # g_function\n",
    "        y_hat, dydz = self.activation(y_hat, self.g_function) # y_hat: changes, dydz: gradient tensor (batch_size, linear_1_out_features)\n",
    "        self.cache['dydz']=dydz\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "    def backward(self, dJdy_hat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the backward function\n",
    "        # layer 2\n",
    "        dJdz = dJdy_hat*self.cache['dydz']\n",
    "        # b2\n",
    "        self.grads['dJdb2'] = sum(dJdz)\n",
    "        # W2\n",
    "        self.grads['dJdW2'] = torch.mm(dJdz.t(), self.cache['z'])\n",
    "        # layer 1\n",
    "        dJdx= torch.mm(dJdz,self.parameters['W2'])*self.cache['dzdz']\n",
    "        # b1\n",
    "        self.grads['dJdb1'] = sum(dJdx)\n",
    "        # W1\n",
    "        self.grads['dJdW1'] = torch.mm(dJdx.t(),self.cache['x'])\n",
    "\n",
    "\n",
    "\n",
    "    def clear_grad_and_cache(self):\n",
    "        for grad in self.grads:\n",
    "            self.grads[grad].zero_()\n",
    "        self.cache = dict()\n",
    "\n",
    "def mse_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: the label tensor (batch_size, linear_2_out_features)\n",
    "        y_hat: the prediction tensor (batch_size, linear_2_out_features)\n",
    "    Return:\n",
    "        loss: scalar of loss\n",
    "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement the mse loss\n",
    "    # batch_size = y.size(0)\n",
    "    # linear_2_out_features = y.size(1)\n",
    "    loss = sum(sum((y_hat-y).pow(2)))\n",
    "    dJdy_hat = 2*(y_hat-y)\n",
    "    # taking mean\n",
    "    loss = loss/y.numel()\n",
    "    dJdy_hat = dJdy_hat/y.numel()\n",
    "    return loss, dJdy_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d02184e6",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "net = MLP(\n",
    "    linear_1_in_features=2,\n",
    "    linear_1_out_features=20,\n",
    "    f_function='relu',\n",
    "    linear_2_in_features=20,\n",
    "    linear_2_out_features=5,\n",
    "    g_function='identity'\n",
    ")\n",
    "x = torch.randn(10, 2)\n",
    "y = torch.randn(10, 5)\n",
    "\n",
    "\n",
    "#Forward: (10,2)@(2,20)@(20,5) -> (10,5)\n",
    "#Backward: dJdy_hat:(10,5)\n",
    "\n",
    "net.clear_grad_and_cache()\n",
    "y_hat = net.forward(x)\n",
    "J, dJdy_hat = mse_loss(y, y_hat)\n",
    "\n",
    "net.backward(dJdy_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84aaf834",
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# compare the result with autograd\n",
    "net_autograd = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('linear1', nn.Linear(2, 20)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('linear2', nn.Linear(20, 5)),\n",
    "    ])\n",
    ")\n",
    "net_autograd.linear1.weight.data = net.parameters['W1']\n",
    "net_autograd.linear1.bias.data = net.parameters['b1']\n",
    "net_autograd.linear2.weight.data = net.parameters['W2']\n",
    "net_autograd.linear2.bias.data = net.parameters['b2']\n",
    "\n",
    "y_hat_autograd = net_autograd(x)\n",
    "\n",
    "J_autograd = F.mse_loss(y_hat_autograd, y)\n",
    "\n",
    "net_autograd.zero_grad()\n",
    "J_autograd.backward()\n",
    "\n",
    "print((net_autograd.linear1.weight.grad.data - net.grads['dJdW1']).norm() < 1e-3)\n",
    "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.weight.grad.data - net.grads['dJdW2']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm()< 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c2c2f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
